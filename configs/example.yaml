DataArguments:

  # DataPathArguments
  dataset_name: ./data/aistage-mrc/train_dataset
  dataset_path: ./data/aistage-mrc
  context_path: preprocess_wiki.json
  overwrite_cache: False
  preprocessing_num_workers: null
  
  # TokenizerArguments
  max_seq_length: 384
  pad_to_max_length: False
  doc_stride: 128
  max_answer_length: 30
  return_token_type_ids: False
  
  # RetrievalArguments
  retrieval_mode: elastic_engine
  retrieval_name: elastic_search
  rebuilt_index: False
  retrieval_tokenizer_name: mecab
  sp_max_features: 50000
  sp_ngram_range: [1,2]
  top_k_retrieval: 35
  use_faiss: False
  eval_retrieval: True
  num_clusters: 64
  
  # ElasticSearchArguments
  index_name: wiki-index
  stopword_path: user_dic/my_stop_dic.txt
  decompound_mode: mixed
  b: 0.5
  k1: 1.3
  es_host_address: localhost:9200
  es_similarity: bm25_similarity
  use_korean_stopwords: False
  use_korean_synonyms: False
  lowercase: False
  nori_readingform: False
  cjk_bigram: False
  decimal_digit: False
  dfr_basic_model: g
  dfr_after_effect: l
  es_normalization: h2
  dfi_measure: standardized
  ib_distribution: ll
  ib_lambda: df
  lmd_mu: 2000
  lmjm_lambda: 0.1
  
ModelArguments:

  # ModelArguments
  model_name_or_path: kiyoung2/roberta-large-qa
  reader_type: extractive
  architectures: RobertForQA
  config_name: null
  tokenizer_name: null
  model_cache_dir: cache
  model_init: basic
  
  # ModelHeadArguments
  model_head: conv
  qa_conv_out_channel: 1024
  qa_conv_input_size: 384
  qa_conv_n_layers: 5
  use_auth_token: True
  
TrainingArguments:

  # HfTrainingArguments
  report_to: wandb
  run_name: run_test
  output_dir: outputs/run_test
  overwrite_output_dir: False
  learning_rate: 2e-6
  do_train: True
  do_eval: True
  do_predict: True # eval_retrieval = True 필수
  evaluation_strategy: epoch
  save_strategy: epoch
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  num_train_epochs: 1
  # eval_steps: 100
  # save_steps: 100
  save_total_limit: 3
  fp16: True
  weight_decay: 0.01
  warmup_steps: 200
  load_best_model_at_end: True
  metric_for_best_model: exact_match
  logging_dir: logs
  lr_scheduler_type: cosine

  # Seq2SeqTrainingArguments
  sortish_sampler: False
  predict_with_generate: False
  generation_max_length: null
  generation_num_beams: null
  
  
ProjectArguments:

  # AnalyzerArguments
  wandb_project: klue_mrc
  checkpoint: null